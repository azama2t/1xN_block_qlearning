# Line Walker - демо Q-learning

Небольшой учебный проект по обучению с подкреплением.  
Агент учится двигаться по одномерной линии от старта до цели, используя табличный Q-learning.  
Есть простой интерфейс на PyQt5, чтобы визуально смотреть, как он начинает "понимать", куда идти.

Этот проект был моим первым опытом работы с Q-learning. Хотелось не столько сделать сложный ИИ, сколько руками потрогать "магию": как из наград и штрафов постепенно рождается поведение, которое я явно не прописывал.

---

## Идея проекта

Мир очень простой:

- Есть линия из N клеток: `0 .. N-1`.
- Агент стартует в клетке `start_state` (обычно 0) значени указывается в `const.py`.
- Цель агента — дойти до клетки `goal_state = N-1`.
- На каждом шаге он может:
  - `0` — идти влево,
  - `1` — идти вправо.

Награды:

- За каждый шаг: `-1` (штраф за то, что тратим время) `сделано для того, чтобы агент как можно быстрее начал проходить игру за N-1 шагов`.
- Если агент дошёл до цели: дополнительно `+10`, эпизод заканчивается.
- Если он ходил слишком долго и превысил `max_steps` (N * 5), эпизод тоже заканчивается без бонуса.

Задача агента: научиться так выбирать действия, чтобы как можно чаще и как можно быстрее доходить до цели.

---

## Почувствуем игру глазами агента

- состояние `state` — просто номер клетки (0, 1, 2, ..., N-1);
- действие `action` — число 0 или 1:
  - 0 — шаг влево,
  - 1 — шаг вправо;
- награда `reward` — число за конкретный шаг:
  - `-1` за каждый ход,
  - `+10`, за прохождение игры;
- после действия он попадает в новое состояние `next_state` и узнаёт, закончился ли эпизод `done`.

Каждый шаг можно записать как строку опыта:

`(state, action, reward, next_state, done)`

На основе таких шагов мы обновляем знания агента (Q таблица).

---

## Что такое Q-таблица

Мы храним знания агента в виде таблицы `Q`:

- строки — состояния (`state`),
- столбцы — действия (`action`).

В коде это `q_table[state][action]`.

Смысл числа `Q(s, a)`:

> Насколько выгодно сделать действие `a` в состоянии `s`,  
> если дальше тоже играть достаточно разумно.

Изначально все значения равны 0. Агент "ничего не знает" и в начале почти случайно исследует мир.

---

## Как мы обновляем Q-таблицу (магия)

После каждого шага у нас есть:

- старое состояние: `state`,
- выбранное действие: `action`,
- награда: `reward`,
- новое состояние: `next_state`,
- флаг конца эпизода: `done`.

Берём старое значение:

`old_value = Q[state][action]`

Считаем целевое значение `target`:

- если эпизод закончился:
  - `target = reward`;
- если нет:
  - смотрим максимальное Q в новом состоянии:
    - `next_max = max(Q[next_state])`;
  - тогда:
    - `target = reward + gamma * next_max`.

И двигаем старое значение в сторону `target`:

`Q[state][action] = old_value + alpha * (target - old_value)`

Где:

- `alpha` — скорость обучения,
- `gamma` — коэффициент "важности будущего".

Так на каждом шаге Q-таблица чуть-чуть подстраивается под опыт агента. Со временем значения стабилизируются.

---

## Как агент выбирает ход по Q-таблице

Мы используем ε-жадную стратегию (epsilon-greedy):

- с вероятностью `epsilon` — выбрать случайное действие (исследование);
- с вероятностью `1 - epsilon` — выбрать действие с максимальным Q (использование знаний).

В коде:

- если `random.random() < epsilon`:
  - берём случайное действие 0 или 1;
- иначе:
  - смотрим `q_table[state]`,
  - находим максимум,
  - выбираем одно из действий с максимальным Q.

Постепенно мы уменьшаем `epsilon`, чтобы агент меньше рандомил и больше опирался на выученную стратегию.

---

## Стек и зависимости

Используется:

- Python 3.x
- Стандартная библиотека:
  - `sys`
  - `random`
- Сторонние библиотеки:
  - `PyQt5` — графический интерфейс

Устанавливать через `pip` нужно только PyQt5, остальное идёт в комплекте с Python.

---

## Установка

1. Клонировать репозиторий:

    git clone https://github.com/azama2t/1xN_block_qlearning
    cd 1xN_block_qlearning

2. (Опционально) создать виртуальное окружение:

    python -m venv venv
    venv\Scripts\activate        # Windows
    source venv/bin/activate     # Linux/macOS

3. Установить зависимости:

    pip install -r requirements.txt

---

## Структура проекта

    .
    ├─ agent.py     # QLearningAgent: выбор действия, обновление Q-таблицы, decay epsilon
    ├─ env.py       # LineEnv: 1D среда (линия 0..N-1, награды, шаги)
    ├─ ui.py        # PyQt5 интерфейс, который соединяет env + agent
    ├─ main.py      # точка входа, создаёт среду и главное окно
    ├─ const.py     # настройки, например start_state
    ├─ README.md    # этот файл
    ├─ USAGE.md     # примеры запуска и сценарии использования
    └─ requirements.txt

---

## Кратко: как всё работает вместе

1. `main.py` создаёт `LineEnv` и `MainWindow`, запускает Qt-приложение.
2. `MainWindow` создаёт `QLearningAgent` и отрисовывает линию из клеток.
3. При запуске:
   - если указан 1 эпизод или поле пустое:
     - запускается один эпизод с анимацией,
     - по таймеру раз в 0.5 секунды:
       - агент выбирает действие по Q-таблице и epsilon,
       - среда делает шаг, возвращает состояние и награду,
       - агент обновляет Q-таблицу,
       - интерфейс перерисовывает позицию агента;
   - если указано `N > 1`:
     - запускается `N` эпизодов в цикле без анимации,
     - в консоль выводится статистика:
       - число успешных эпизодов,
       - среднее число шагов,
       - текущий `epsilon`,
       - сама Q-таблица.

---

## Идеи для развития

- Кнопка "Сбросить обучение" с обнулением Q-таблицы и epsilon.
- Добавить графики:
  - длина эпизода от номера эпизода,
  - процент побед от номера эпизода.
- Добавить "ямы" с большим отрицательным reward.
- Перенести среду в формат, похожий на OpenAI Gym.
- Поиграться с разными схемами наград и параметрами `alpha`, `gamma`, `epsilon`.

Проект маленький и простой, но через него удобно почувствовать, как Q-learning шаг за шагом превращает рандомного бота в того, кто стабильно делает "правильные" ходы.
