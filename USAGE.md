# Usage

Ниже несколько базовых сценариев использования проекта.

---

## Запуск приложения

Из корня репозитория:

    python main.py

Откроется окно с линией из клеток, где:

- зелёная клетка с буквой `A` — текущая позиция агента,
- голубая клетка с буквой `G` — цель.

Внизу есть:

- поле ввода количества эпизодов,
- кнопка `Запустить`,
- кнопка `Сбросить`,
- строка статуса.

---

## Один эпизод с анимацией

1. Оставь поле `кол-во эпизодов` пустым или введи `1`.
2. Нажми `Запустить`.

Что произойдёт:

- среда сбрасывается,
- начинается новый эпизод,
- каждые 0.5 секунды:
  - агент выбирает действие,
  - среда делает шаг,
  - обновляется Q-таблица,
  - интерфейс перерисовывает позицию агента,
- когда эпизод заканчивается:
  - таймер останавливается,
  - в консоль выводится лог вида:

        [SINGLE] episode=42, steps=3, success=True, epsilon=0.100

        === Q-таблица ===
        state 0: L=  4.580 | R=  6.200
        state 1: L=  4.580 | R=  8.000
        state 2: L=  6.200 | R= 10.000
        state 3: L=  0.000 | R=  0.000
        =================

  - `epsilon` немного уменьшается.

Так можно наблюдать, как после множества запусков агент всё чаще идёт прямо к цели и эпизоды становятся короче.

---

## Быстрая тренировка на многих эпизодах

1. Введи в поле `кол-во эпизодов` число больше 1, например `100` или `1000`.
2. Нажми `Запустить`.

Что произойдёт:

- запустится цикл на `N` эпизодов без анимации,
- для каждого эпизода:
  - агент учится на каждом шаге,
  - в конце эпизода немного уменьшается `epsilon`,
- по завершении `N` эпизодов в консоль выводится статистика, например:

        [FAST] episodes=1000, successes=997, success_rate=99.7%, avg_steps=3.02, epsilon=0.105

        === Q-таблица ===
        state 0: L=  4.580 | R=  6.200
        state 1: L=  4.580 | R=  8.000
        state 2: L=  6.200 | R= 10.000
        state 3: L=  0.000 | R=  0.000
        =================

- в строке статуса внутри окна также кратко отображается результат.

После быстрой тренировки можно снова запустить один эпизод с анимацией и посмотреть, как агент уже "знает", куда идти.

---

## Сброс

- Кнопка `Сбросить` сейчас:
  - сбрасывает состояние среды (агент возвращается в стартовую позицию),
  - обнуляет счётчики эпизодов и шагов,
  - не обнуляет Q-таблицу (агент сохраняет обученные веса).

Если нужно сбрасывать именно обучение (Q-таблицу и epsilon), можно:

- добавить в `QLearningAgent` метод `reset_q_table()` и вызывать его из обработчика кнопки,
- или вручную пересоздавать объект `QLearningAgent` в `MainWindow`.

---

## Настройки

Некоторые параметры можно менять в коде:

- в `env.py`:
  - `length` — длина линии,
  - `max_steps` — максимум шагов за эпизод,
  - награды за шаг и за достижение цели;
- в `agent.py`:
  - `alpha` — скорость обучения,
  - `gamma` — дисконт будущих наград,
  - `epsilon` — начальная степень случайности,
  - логика уменьшения `epsilon` в `decay_epsilon`.

Через эти параметры удобно экспериментировать и смотреть, как меняется скорость обучения и качество политики.
